{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "chunk_size = 1_000_000  # Number of rows per chunk\n",
    "chunks = pd.read_csv('combined_wesad_data.csv', chunksize=chunk_size)\n",
    "\n",
    "df = next(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ekanshsahu/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "# Prepare data (reshaping for LSTM: [samples, timesteps, features])\n",
    "X = df.drop(columns=['label', 'participant', 'time'])\n",
    "y = df['label']\n",
    "\n",
    "columns_to_scale = ['heart_rate', 'heart_rate_ma', 'accel_x', 'accel_y', 'accel_z', 'accel_magnitude', 'temperature']\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X[columns_to_scale] = scaler.fit_transform(X[columns_to_scale])\n",
    "\n",
    "# Chunk data into sequences (e.g., 10 time steps per sample)\n",
    "def create_sequences(data, labels, sequence_length=10):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        X_seq.append(data.iloc[i:i+sequence_length].values)\n",
    "        y_seq.append(labels.iloc[i+sequence_length])  # Use label at the end of the sequence\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "X_seq, y_seq = create_sequences(X, y, sequence_length=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode labels for classification\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_seq_encoded = encoder.fit_transform(y_seq.reshape(-1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "chunk_size = 1_000_000  # Number of rows per chunk\n",
    "chunks = pd.read_csv('combined_wesad_data.csv', chunksize=chunk_size)\n",
    "\n",
    "df = next(chunks)\n",
    "\n",
    "# Drop participant column and handle missing data\n",
    "df = df.drop(columns=['participant']).dropna()\n",
    "\n",
    "# Normalize the data (excluding the label)\n",
    "scaler = StandardScaler()\n",
    "features = df.drop(columns=['label']).values\n",
    "labels = df['label'].values\n",
    "features = scaler.fit_transform(features)\n",
    "\n",
    "# 2. Create overlapping sequences (patches)\n",
    "def create_sequences(data, labels, sequence_length=50):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        X.append(data[i:i+sequence_length])  # Create a sequence of 'sequence_length'\n",
    "        y.append(labels[i+sequence_length])  # Label corresponds to the end of the sequence\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "sequence_length = 50\n",
    "X, y = create_sequences(features, labels, sequence_length)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=42, stratify=y)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# 3. Create DataLoader\n",
    "batch_size = 64\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 4. PatchTST Model Definition\n",
    "class PatchTST(nn.Module):\n",
    "    def __init__(self, input_dim, patch_size, d_model, n_heads, num_layers, num_classes, dropout=0.1):\n",
    "        super(PatchTST, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.embedding = nn.Linear(patch_size * input_dim, d_model)  # Patch embedding\n",
    "        self.positional_encoding = nn.Parameter(torch.randn(1, 500, d_model))  # Positional encoding\n",
    "        \n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=n_heads, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        self.fc = nn.Linear(d_model, num_classes)  # Output layer for classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, D = x.shape  # Batch, Time, Features\n",
    "        num_patches = T // self.patch_size\n",
    "        x = x[:, :num_patches * self.patch_size, :]  # Ensure divisible by patch size\n",
    "        x = x.view(B, num_patches, -1)  # Reshape into patches\n",
    "\n",
    "        # Patch embedding and positional encoding\n",
    "        x = self.embedding(x)\n",
    "        x += self.positional_encoding[:, :x.size(1), :]  # Add positional encoding\n",
    "\n",
    "        # Transformer Encoder\n",
    "        x = self.transformer_encoder(x)\n",
    "\n",
    "        # Classification token (use the mean of all tokens)\n",
    "        x = x.mean(dim=1)  # Average across the sequence length\n",
    "        out = self.fc(x)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# 5. Initialize Model, Loss, and Optimizer\n",
    "input_dim = X_train.shape[2]  # Number of features\n",
    "patch_size = 10\n",
    "d_model = 64\n",
    "n_heads = 4\n",
    "num_layers = 3\n",
    "num_classes = len(np.unique(y))  # Number of unique labels (0-7)\n",
    "dropout = 0.1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = PatchTST(input_dim, patch_size, d_model, n_heads, num_layers, num_classes, dropout).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# 6. Training Loop\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch_idx, (X_batch, y_batch) in enumerate(train_loader):\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Print update every 10 batches\n",
    "        if (batch_idx + 1) % 100 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Batch {batch_idx+1}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {train_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# 7. Evaluate Model\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        outputs = model(X_batch)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += y_batch.size(0)\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'plotly'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplotly\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'plotly'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ekanshsahu/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Batch 100/10937, Loss: 0.0885\n",
      "Epoch 1/5, Batch 200/10937, Loss: 0.0091\n",
      "Epoch 1/5, Batch 300/10937, Loss: 0.0139\n",
      "Epoch 1/5, Batch 400/10937, Loss: 0.0069\n",
      "Epoch 1/5, Batch 500/10937, Loss: 0.0181\n",
      "Epoch 1/5, Batch 600/10937, Loss: 0.0215\n",
      "Epoch 1/5, Batch 700/10937, Loss: 0.0006\n",
      "Epoch 1/5, Batch 800/10937, Loss: 0.0172\n",
      "Epoch 1/5, Batch 900/10937, Loss: 0.0170\n",
      "Epoch 1/5, Batch 1000/10937, Loss: 0.0031\n",
      "Epoch 1/5, Batch 1100/10937, Loss: 0.0099\n",
      "Epoch 1/5, Batch 1200/10937, Loss: 0.0091\n",
      "Epoch 1/5, Batch 1300/10937, Loss: 0.0112\n",
      "Epoch 1/5, Batch 1400/10937, Loss: 0.0065\n",
      "Epoch 1/5, Batch 1500/10937, Loss: 0.0163\n",
      "Epoch 1/5, Batch 1600/10937, Loss: 0.0063\n",
      "Epoch 1/5, Batch 1700/10937, Loss: 0.0222\n",
      "Epoch 1/5, Batch 1800/10937, Loss: 0.0041\n",
      "Epoch 1/5, Batch 1900/10937, Loss: 0.0194\n",
      "Epoch 1/5, Batch 2000/10937, Loss: 0.0331\n",
      "Epoch 1/5, Batch 2100/10937, Loss: 0.0132\n",
      "Epoch 1/5, Batch 2200/10937, Loss: 0.0192\n",
      "Epoch 1/5, Batch 2300/10937, Loss: 0.0058\n",
      "Epoch 1/5, Batch 2400/10937, Loss: 0.0142\n",
      "Epoch 1/5, Batch 2500/10937, Loss: 0.0001\n",
      "Epoch 1/5, Batch 2600/10937, Loss: 0.0524\n",
      "Epoch 1/5, Batch 2700/10937, Loss: 0.0686\n",
      "Epoch 1/5, Batch 2800/10937, Loss: 0.0007\n",
      "Epoch 1/5, Batch 2900/10937, Loss: 0.0008\n",
      "Epoch 1/5, Batch 3000/10937, Loss: 0.0116\n",
      "Epoch 1/5, Batch 3100/10937, Loss: 0.0214\n",
      "Epoch 1/5, Batch 3200/10937, Loss: 0.0002\n",
      "Epoch 1/5, Batch 3300/10937, Loss: 0.0069\n",
      "Epoch 1/5, Batch 3400/10937, Loss: 0.0107\n",
      "Epoch 1/5, Batch 3500/10937, Loss: 0.0773\n",
      "Epoch 1/5, Batch 3600/10937, Loss: 0.0069\n",
      "Epoch 1/5, Batch 3700/10937, Loss: 0.0021\n",
      "Epoch 1/5, Batch 3800/10937, Loss: 0.0182\n",
      "Epoch 1/5, Batch 3900/10937, Loss: 0.0025\n",
      "Epoch 1/5, Batch 4000/10937, Loss: 0.0014\n",
      "Epoch 1/5, Batch 4100/10937, Loss: 0.0010\n",
      "Epoch 1/5, Batch 4200/10937, Loss: 0.0651\n",
      "Epoch 1/5, Batch 4300/10937, Loss: 0.1064\n",
      "Epoch 1/5, Batch 4400/10937, Loss: 0.0185\n",
      "Epoch 1/5, Batch 4500/10937, Loss: 0.0015\n",
      "Epoch 1/5, Batch 4600/10937, Loss: 0.0998\n",
      "Epoch 1/5, Batch 4700/10937, Loss: 0.2454\n",
      "Epoch 1/5, Batch 4800/10937, Loss: 0.0691\n",
      "Epoch 1/5, Batch 4900/10937, Loss: 0.0298\n",
      "Epoch 1/5, Batch 5000/10937, Loss: 0.0468\n",
      "Epoch 1/5, Batch 5100/10937, Loss: 0.2790\n",
      "Epoch 1/5, Batch 5200/10937, Loss: 0.2003\n",
      "Epoch 1/5, Batch 5300/10937, Loss: 0.0758\n",
      "Epoch 1/5, Batch 5400/10937, Loss: 0.1795\n",
      "Epoch 1/5, Batch 5500/10937, Loss: 0.1247\n",
      "Epoch 1/5, Batch 5600/10937, Loss: 0.1500\n",
      "Epoch 1/5, Batch 5700/10937, Loss: 0.2861\n",
      "Epoch 1/5, Batch 5800/10937, Loss: 0.1324\n",
      "Epoch 1/5, Batch 5900/10937, Loss: 0.0830\n",
      "Epoch 1/5, Batch 6000/10937, Loss: 0.2127\n",
      "Epoch 1/5, Batch 6100/10937, Loss: 0.1216\n",
      "Epoch 1/5, Batch 6200/10937, Loss: 0.1725\n",
      "Epoch 1/5, Batch 6300/10937, Loss: 0.1149\n",
      "Epoch 1/5, Batch 6400/10937, Loss: 0.0943\n",
      "Epoch 1/5, Batch 6500/10937, Loss: 0.0970\n",
      "Epoch 1/5, Batch 6600/10937, Loss: 0.2387\n",
      "Epoch 1/5, Batch 6700/10937, Loss: 0.0739\n",
      "Epoch 1/5, Batch 6800/10937, Loss: 0.2202\n",
      "Epoch 1/5, Batch 6900/10937, Loss: 0.2853\n",
      "Epoch 1/5, Batch 7000/10937, Loss: 0.2141\n",
      "Epoch 1/5, Batch 7100/10937, Loss: 0.1079\n",
      "Epoch 1/5, Batch 7200/10937, Loss: 0.2879\n",
      "Epoch 1/5, Batch 7300/10937, Loss: 0.2969\n",
      "Epoch 1/5, Batch 7400/10937, Loss: 0.2955\n",
      "Epoch 1/5, Batch 7500/10937, Loss: 0.5613\n",
      "Epoch 1/5, Batch 7600/10937, Loss: 0.3483\n",
      "Epoch 1/5, Batch 7700/10937, Loss: 0.3427\n",
      "Epoch 1/5, Batch 7800/10937, Loss: 0.3741\n",
      "Epoch 1/5, Batch 7900/10937, Loss: 0.5036\n",
      "Epoch 1/5, Batch 8000/10937, Loss: 0.2386\n",
      "Epoch 1/5, Batch 8100/10937, Loss: 0.7252\n",
      "Epoch 1/5, Batch 8200/10937, Loss: 0.2436\n",
      "Epoch 1/5, Batch 8300/10937, Loss: 0.1362\n",
      "Epoch 1/5, Batch 8400/10937, Loss: 0.7041\n",
      "Epoch 1/5, Batch 8500/10937, Loss: 0.0831\n",
      "Epoch 1/5, Batch 8600/10937, Loss: 0.2876\n",
      "Epoch 1/5, Batch 8700/10937, Loss: 0.0711\n",
      "Epoch 1/5, Batch 8800/10937, Loss: 0.0707\n",
      "Epoch 1/5, Batch 8900/10937, Loss: 0.3240\n",
      "Epoch 1/5, Batch 9000/10937, Loss: 0.2335\n",
      "Epoch 1/5, Batch 9100/10937, Loss: 0.0414\n",
      "Epoch 1/5, Batch 9200/10937, Loss: 0.1751\n",
      "Epoch 1/5, Batch 9300/10937, Loss: 0.1810\n",
      "Epoch 1/5, Batch 9400/10937, Loss: 0.0187\n",
      "Epoch 1/5, Batch 9500/10937, Loss: 0.0246\n",
      "Epoch 1/5, Batch 9600/10937, Loss: 0.0889\n",
      "Epoch 1/5, Batch 9700/10937, Loss: 0.1030\n",
      "Epoch 1/5, Batch 9800/10937, Loss: 0.0247\n",
      "Epoch 1/5, Batch 9900/10937, Loss: 0.0936\n",
      "Epoch 1/5, Batch 10000/10937, Loss: 0.0237\n",
      "Epoch 1/5, Batch 10100/10937, Loss: 0.0413\n",
      "Epoch 1/5, Batch 10200/10937, Loss: 0.0786\n",
      "Epoch 1/5, Batch 10300/10937, Loss: 0.1531\n",
      "Epoch 1/5, Batch 10400/10937, Loss: 0.1391\n",
      "Epoch 1/5, Batch 10500/10937, Loss: 0.1624\n",
      "Epoch 1/5, Batch 10600/10937, Loss: 0.0264\n",
      "Epoch 1/5, Batch 10700/10937, Loss: 0.1681\n",
      "Epoch 1/5, Batch 10800/10937, Loss: 0.1207\n",
      "Epoch 1/5, Batch 10900/10937, Loss: 0.0955\n",
      "Epoch 1/5, Loss: 0.1304\n",
      "Epoch 2/5, Batch 100/10937, Loss: 0.0903\n",
      "Epoch 2/5, Batch 200/10937, Loss: 0.0231\n",
      "Epoch 2/5, Batch 300/10937, Loss: 0.0938\n",
      "Epoch 2/5, Batch 400/10937, Loss: 0.1593\n",
      "Epoch 2/5, Batch 500/10937, Loss: 0.0597\n",
      "Epoch 2/5, Batch 600/10937, Loss: 0.1124\n",
      "Epoch 2/5, Batch 700/10937, Loss: 0.0541\n",
      "Epoch 2/5, Batch 800/10937, Loss: 0.1027\n",
      "Epoch 2/5, Batch 900/10937, Loss: 0.0610\n",
      "Epoch 2/5, Batch 1000/10937, Loss: 0.2964\n",
      "Epoch 2/5, Batch 1100/10937, Loss: 0.2329\n",
      "Epoch 2/5, Batch 1200/10937, Loss: 0.2337\n",
      "Epoch 2/5, Batch 1300/10937, Loss: 0.0929\n",
      "Epoch 2/5, Batch 1400/10937, Loss: 0.1363\n",
      "Epoch 2/5, Batch 1500/10937, Loss: 0.0945\n",
      "Epoch 2/5, Batch 1600/10937, Loss: 0.0104\n",
      "Epoch 2/5, Batch 1700/10937, Loss: 0.0681\n",
      "Epoch 2/5, Batch 1800/10937, Loss: 0.1001\n",
      "Epoch 2/5, Batch 1900/10937, Loss: 0.0847\n",
      "Epoch 2/5, Batch 2000/10937, Loss: 0.1242\n",
      "Epoch 2/5, Batch 2100/10937, Loss: 0.1624\n",
      "Epoch 2/5, Batch 2200/10937, Loss: 0.0212\n",
      "Epoch 2/5, Batch 2300/10937, Loss: 0.0408\n",
      "Epoch 2/5, Batch 2400/10937, Loss: 0.1628\n",
      "Epoch 2/5, Batch 2500/10937, Loss: 0.0198\n",
      "Epoch 2/5, Batch 2600/10937, Loss: 0.0680\n",
      "Epoch 2/5, Batch 2700/10937, Loss: 0.0210\n",
      "Epoch 2/5, Batch 2800/10937, Loss: 0.0281\n",
      "Epoch 2/5, Batch 2900/10937, Loss: 0.0612\n",
      "Epoch 2/5, Batch 3000/10937, Loss: 0.0530\n",
      "Epoch 2/5, Batch 3100/10937, Loss: 0.1827\n",
      "Epoch 2/5, Batch 3200/10937, Loss: 0.0893\n",
      "Epoch 2/5, Batch 3300/10937, Loss: 0.0202\n",
      "Epoch 2/5, Batch 3400/10937, Loss: 0.0975\n",
      "Epoch 2/5, Batch 3500/10937, Loss: 0.0662\n",
      "Epoch 2/5, Batch 3600/10937, Loss: 0.0126\n",
      "Epoch 2/5, Batch 3700/10937, Loss: 0.0146\n",
      "Epoch 2/5, Batch 3800/10937, Loss: 0.0096\n",
      "Epoch 2/5, Batch 3900/10937, Loss: 0.0601\n",
      "Epoch 2/5, Batch 4000/10937, Loss: 0.0173\n",
      "Epoch 2/5, Batch 4100/10937, Loss: 0.0059\n",
      "Epoch 2/5, Batch 4200/10937, Loss: 0.0614\n",
      "Epoch 2/5, Batch 4300/10937, Loss: 0.0878\n",
      "Epoch 2/5, Batch 4400/10937, Loss: 0.1289\n",
      "Epoch 2/5, Batch 4500/10937, Loss: 0.2067\n",
      "Epoch 2/5, Batch 4600/10937, Loss: 0.0960\n",
      "Epoch 2/5, Batch 4700/10937, Loss: 0.1249\n",
      "Epoch 2/5, Batch 4800/10937, Loss: 0.1205\n",
      "Epoch 2/5, Batch 4900/10937, Loss: 0.0897\n",
      "Epoch 2/5, Batch 5000/10937, Loss: 0.0885\n",
      "Epoch 2/5, Batch 5100/10937, Loss: 0.0877\n",
      "Epoch 2/5, Batch 5200/10937, Loss: 0.1618\n",
      "Epoch 2/5, Batch 5300/10937, Loss: 0.1903\n",
      "Epoch 2/5, Batch 5400/10937, Loss: 0.1738\n",
      "Epoch 2/5, Batch 5500/10937, Loss: 0.1145\n",
      "Epoch 2/5, Batch 5600/10937, Loss: 0.1369\n",
      "Epoch 2/5, Batch 5700/10937, Loss: 0.1004\n",
      "Epoch 2/5, Batch 5800/10937, Loss: 0.0845\n",
      "Epoch 2/5, Batch 5900/10937, Loss: 0.1426\n",
      "Epoch 2/5, Batch 6000/10937, Loss: 0.0642\n",
      "Epoch 2/5, Batch 6100/10937, Loss: 0.1049\n",
      "Epoch 2/5, Batch 6200/10937, Loss: 0.1163\n",
      "Epoch 2/5, Batch 6300/10937, Loss: 0.0651\n",
      "Epoch 2/5, Batch 6400/10937, Loss: 0.1099\n",
      "Epoch 2/5, Batch 6500/10937, Loss: 0.0832\n",
      "Epoch 2/5, Batch 6600/10937, Loss: 0.1767\n",
      "Epoch 2/5, Batch 6700/10937, Loss: 0.1523\n",
      "Epoch 2/5, Batch 6800/10937, Loss: 0.1162\n",
      "Epoch 2/5, Batch 6900/10937, Loss: 0.1365\n",
      "Epoch 2/5, Batch 7000/10937, Loss: 0.0724\n",
      "Epoch 2/5, Batch 7100/10937, Loss: 0.1289\n",
      "Epoch 2/5, Batch 7200/10937, Loss: 0.1678\n",
      "Epoch 2/5, Batch 7300/10937, Loss: 0.2051\n",
      "Epoch 2/5, Batch 7400/10937, Loss: 0.1424\n",
      "Epoch 2/5, Batch 7500/10937, Loss: 0.0972\n",
      "Epoch 2/5, Batch 7600/10937, Loss: 0.0968\n",
      "Epoch 2/5, Batch 7700/10937, Loss: 0.0950\n",
      "Epoch 2/5, Batch 7800/10937, Loss: 0.1236\n",
      "Epoch 2/5, Batch 7900/10937, Loss: 0.1951\n",
      "Epoch 2/5, Batch 8000/10937, Loss: 0.2031\n",
      "Epoch 2/5, Batch 8100/10937, Loss: 0.1427\n",
      "Epoch 2/5, Batch 8200/10937, Loss: 0.1368\n",
      "Epoch 2/5, Batch 8300/10937, Loss: 0.1774\n",
      "Epoch 2/5, Batch 8400/10937, Loss: 0.1988\n",
      "Epoch 2/5, Batch 8500/10937, Loss: 0.1878\n",
      "Epoch 2/5, Batch 8600/10937, Loss: 0.1294\n",
      "Epoch 2/5, Batch 8700/10937, Loss: 0.1740\n",
      "Epoch 2/5, Batch 8800/10937, Loss: 0.1280\n",
      "Epoch 2/5, Batch 8900/10937, Loss: 0.1430\n",
      "Epoch 2/5, Batch 9000/10937, Loss: 0.0966\n",
      "Epoch 2/5, Batch 9100/10937, Loss: 0.1624\n",
      "Epoch 2/5, Batch 9200/10937, Loss: 0.1916\n",
      "Epoch 2/5, Batch 9300/10937, Loss: 0.0619\n",
      "Epoch 2/5, Batch 9400/10937, Loss: 0.1382\n",
      "Epoch 2/5, Batch 9500/10937, Loss: 0.0757\n",
      "Epoch 2/5, Batch 9600/10937, Loss: 0.1565\n",
      "Epoch 2/5, Batch 9700/10937, Loss: 0.1376\n",
      "Epoch 2/5, Batch 9800/10937, Loss: 0.1629\n",
      "Epoch 2/5, Batch 9900/10937, Loss: 0.1552\n",
      "Epoch 2/5, Batch 10000/10937, Loss: 0.2528\n",
      "Epoch 2/5, Batch 10100/10937, Loss: 0.1449\n",
      "Epoch 2/5, Batch 10200/10937, Loss: 0.1257\n",
      "Epoch 2/5, Batch 10300/10937, Loss: 0.1739\n",
      "Epoch 2/5, Batch 10400/10937, Loss: 0.0681\n",
      "Epoch 2/5, Batch 10500/10937, Loss: 0.0903\n",
      "Epoch 2/5, Batch 10600/10937, Loss: 0.1271\n",
      "Epoch 2/5, Batch 10700/10937, Loss: 0.2122\n",
      "Epoch 2/5, Batch 10800/10937, Loss: 0.0624\n",
      "Epoch 2/5, Batch 10900/10937, Loss: 0.1271\n",
      "Epoch 2/5, Loss: 0.1177\n",
      "Epoch 3/5, Batch 100/10937, Loss: 0.1568\n",
      "Epoch 3/5, Batch 200/10937, Loss: 0.2276\n",
      "Epoch 3/5, Batch 300/10937, Loss: 0.1323\n",
      "Epoch 3/5, Batch 400/10937, Loss: 0.0663\n",
      "Epoch 3/5, Batch 500/10937, Loss: 0.1479\n",
      "Epoch 3/5, Batch 600/10937, Loss: 0.2320\n",
      "Epoch 3/5, Batch 700/10937, Loss: 0.2025\n",
      "Epoch 3/5, Batch 800/10937, Loss: 0.1265\n",
      "Epoch 3/5, Batch 900/10937, Loss: 0.1825\n",
      "Epoch 3/5, Batch 1000/10937, Loss: 0.2321\n",
      "Epoch 3/5, Batch 1100/10937, Loss: 0.2880\n",
      "Epoch 3/5, Batch 1200/10937, Loss: 0.1661\n",
      "Epoch 3/5, Batch 1300/10937, Loss: 0.1728\n",
      "Epoch 3/5, Batch 1400/10937, Loss: 0.1946\n",
      "Epoch 3/5, Batch 1500/10937, Loss: 0.1997\n",
      "Epoch 3/5, Batch 1600/10937, Loss: 0.2032\n",
      "Epoch 3/5, Batch 1700/10937, Loss: 0.2646\n",
      "Epoch 3/5, Batch 1800/10937, Loss: 0.3545\n",
      "Epoch 3/5, Batch 1900/10937, Loss: 0.1040\n",
      "Epoch 3/5, Batch 2000/10937, Loss: 0.2459\n",
      "Epoch 3/5, Batch 2100/10937, Loss: 0.1501\n",
      "Epoch 3/5, Batch 2200/10937, Loss: 0.0540\n",
      "Epoch 3/5, Batch 2300/10937, Loss: 0.1754\n",
      "Epoch 3/5, Batch 2400/10937, Loss: 0.1765\n",
      "Epoch 3/5, Batch 2500/10937, Loss: 0.1783\n",
      "Epoch 3/5, Batch 2600/10937, Loss: 0.1484\n",
      "Epoch 3/5, Batch 2700/10937, Loss: 0.1170\n",
      "Epoch 3/5, Batch 2800/10937, Loss: 0.1371\n",
      "Epoch 3/5, Batch 2900/10937, Loss: 0.1835\n",
      "Epoch 3/5, Batch 3000/10937, Loss: 0.0644\n",
      "Epoch 3/5, Batch 3100/10937, Loss: 0.0904\n",
      "Epoch 3/5, Batch 3200/10937, Loss: 0.1819\n",
      "Epoch 3/5, Batch 3300/10937, Loss: 0.1438\n",
      "Epoch 3/5, Batch 3400/10937, Loss: 0.1665\n",
      "Epoch 3/5, Batch 3500/10937, Loss: 0.1475\n",
      "Epoch 3/5, Batch 3600/10937, Loss: 0.0697\n",
      "Epoch 3/5, Batch 3700/10937, Loss: 0.1616\n",
      "Epoch 3/5, Batch 3800/10937, Loss: 0.1218\n",
      "Epoch 3/5, Batch 3900/10937, Loss: 0.1650\n",
      "Epoch 3/5, Batch 4000/10937, Loss: 0.1877\n",
      "Epoch 3/5, Batch 4100/10937, Loss: 0.1809\n",
      "Epoch 3/5, Batch 4200/10937, Loss: 0.2498\n",
      "Epoch 3/5, Batch 4300/10937, Loss: 0.1537\n",
      "Epoch 3/5, Batch 4400/10937, Loss: 0.1540\n",
      "Epoch 3/5, Batch 4500/10937, Loss: 0.1268\n",
      "Epoch 3/5, Batch 4600/10937, Loss: 0.1224\n",
      "Epoch 3/5, Batch 4700/10937, Loss: 0.1600\n",
      "Epoch 3/5, Batch 4800/10937, Loss: 0.1898\n",
      "Epoch 3/5, Batch 4900/10937, Loss: 0.1892\n",
      "Epoch 3/5, Batch 5000/10937, Loss: 0.2263\n",
      "Epoch 3/5, Batch 5100/10937, Loss: 0.1530\n",
      "Epoch 3/5, Batch 5200/10937, Loss: 0.2475\n",
      "Epoch 3/5, Batch 5300/10937, Loss: 0.1631\n",
      "Epoch 3/5, Batch 5400/10937, Loss: 0.2084\n",
      "Epoch 3/5, Batch 5500/10937, Loss: 0.2198\n",
      "Epoch 3/5, Batch 5600/10937, Loss: 0.3317\n",
      "Epoch 3/5, Batch 5700/10937, Loss: 0.2025\n",
      "Epoch 3/5, Batch 5800/10937, Loss: 0.1199\n",
      "Epoch 3/5, Batch 5900/10937, Loss: 0.4547\n",
      "Epoch 3/5, Batch 6000/10937, Loss: 0.1486\n",
      "Epoch 3/5, Batch 6100/10937, Loss: 0.1203\n",
      "Epoch 3/5, Batch 6200/10937, Loss: 0.1623\n",
      "Epoch 3/5, Batch 6300/10937, Loss: 0.5330\n",
      "Epoch 3/5, Batch 6400/10937, Loss: 0.4975\n",
      "Epoch 3/5, Batch 6500/10937, Loss: 0.4589\n",
      "Epoch 3/5, Batch 6600/10937, Loss: 0.4911\n",
      "Epoch 3/5, Batch 6700/10937, Loss: 0.5833\n",
      "Epoch 3/5, Batch 6800/10937, Loss: 0.6247\n",
      "Epoch 3/5, Batch 6900/10937, Loss: 0.4465\n",
      "Epoch 3/5, Batch 7000/10937, Loss: 0.5296\n",
      "Epoch 3/5, Batch 7100/10937, Loss: 0.3818\n",
      "Epoch 3/5, Batch 7200/10937, Loss: 0.5420\n",
      "Epoch 3/5, Batch 7300/10937, Loss: 0.4646\n",
      "Epoch 3/5, Batch 7400/10937, Loss: 0.5041\n",
      "Epoch 3/5, Batch 7500/10937, Loss: 0.6201\n",
      "Epoch 3/5, Batch 7600/10937, Loss: 0.5324\n",
      "Epoch 3/5, Batch 7700/10937, Loss: 0.4737\n",
      "Epoch 3/5, Batch 7800/10937, Loss: 0.5252\n",
      "Epoch 3/5, Batch 7900/10937, Loss: 0.5853\n",
      "Epoch 3/5, Batch 8000/10937, Loss: 0.5820\n",
      "Epoch 3/5, Batch 8100/10937, Loss: 0.4456\n",
      "Epoch 3/5, Batch 8200/10937, Loss: 0.4670\n",
      "Epoch 3/5, Batch 8300/10937, Loss: 0.4842\n",
      "Epoch 3/5, Batch 8400/10937, Loss: 0.6087\n",
      "Epoch 3/5, Batch 8500/10937, Loss: 0.3954\n",
      "Epoch 3/5, Batch 8600/10937, Loss: 0.5442\n",
      "Epoch 3/5, Batch 8700/10937, Loss: 0.6216\n",
      "Epoch 3/5, Batch 8800/10937, Loss: 0.4819\n",
      "Epoch 3/5, Batch 8900/10937, Loss: 0.5520\n",
      "Epoch 3/5, Batch 9000/10937, Loss: 0.4844\n",
      "Epoch 3/5, Batch 9100/10937, Loss: 0.4844\n",
      "Epoch 3/5, Batch 9200/10937, Loss: 0.5064\n",
      "Epoch 3/5, Batch 9300/10937, Loss: 0.3899\n",
      "Epoch 3/5, Batch 9400/10937, Loss: 0.4445\n",
      "Epoch 3/5, Batch 9500/10937, Loss: 0.5449\n",
      "Epoch 3/5, Batch 9600/10937, Loss: 0.4157\n",
      "Epoch 3/5, Batch 9700/10937, Loss: 0.4401\n",
      "Epoch 3/5, Batch 9800/10937, Loss: 0.5255\n",
      "Epoch 3/5, Batch 9900/10937, Loss: 0.5484\n",
      "Epoch 3/5, Batch 10000/10937, Loss: 0.4481\n",
      "Epoch 3/5, Batch 10100/10937, Loss: 0.5636\n",
      "Epoch 3/5, Batch 10200/10937, Loss: 0.5034\n",
      "Epoch 3/5, Batch 10300/10937, Loss: 0.4102\n",
      "Epoch 3/5, Batch 10400/10937, Loss: 0.6162\n",
      "Epoch 3/5, Batch 10500/10937, Loss: 0.6124\n",
      "Epoch 3/5, Batch 10600/10937, Loss: 0.6236\n",
      "Epoch 3/5, Batch 10700/10937, Loss: 0.4437\n",
      "Epoch 3/5, Batch 10800/10937, Loss: 0.5656\n",
      "Epoch 3/5, Batch 10900/10937, Loss: 0.4232\n",
      "Epoch 3/5, Loss: 0.3172\n",
      "Epoch 4/5, Batch 100/10937, Loss: 0.3981\n",
      "Epoch 4/5, Batch 200/10937, Loss: 0.5258\n",
      "Epoch 4/5, Batch 300/10937, Loss: 0.5449\n",
      "Epoch 4/5, Batch 400/10937, Loss: 0.5440\n",
      "Epoch 4/5, Batch 500/10937, Loss: 0.4713\n",
      "Epoch 4/5, Batch 600/10937, Loss: 0.5902\n",
      "Epoch 4/5, Batch 700/10937, Loss: 0.3944\n",
      "Epoch 4/5, Batch 800/10937, Loss: 0.4830\n",
      "Epoch 4/5, Batch 900/10937, Loss: 0.5843\n",
      "Epoch 4/5, Batch 1000/10937, Loss: 0.5084\n",
      "Epoch 4/5, Batch 1100/10937, Loss: 0.5982\n",
      "Epoch 4/5, Batch 1200/10937, Loss: 0.5274\n",
      "Epoch 4/5, Batch 1300/10937, Loss: 0.6204\n",
      "Epoch 4/5, Batch 1400/10937, Loss: 0.5234\n",
      "Epoch 4/5, Batch 1500/10937, Loss: 0.5031\n",
      "Epoch 4/5, Batch 1600/10937, Loss: 0.4873\n",
      "Epoch 4/5, Batch 1700/10937, Loss: 0.5053\n",
      "Epoch 4/5, Batch 1800/10937, Loss: 0.4800\n",
      "Epoch 4/5, Batch 1900/10937, Loss: 0.3505\n",
      "Epoch 4/5, Batch 2000/10937, Loss: 0.5512\n",
      "Epoch 4/5, Batch 2100/10937, Loss: 0.6030\n",
      "Epoch 4/5, Batch 2200/10937, Loss: 0.3872\n",
      "Epoch 4/5, Batch 2300/10937, Loss: 0.1416\n",
      "Epoch 4/5, Batch 2400/10937, Loss: 0.1521\n",
      "Epoch 4/5, Batch 2500/10937, Loss: 0.2042\n",
      "Epoch 4/5, Batch 2600/10937, Loss: 0.0767\n",
      "Epoch 4/5, Batch 2700/10937, Loss: 0.1277\n",
      "Epoch 4/5, Batch 2800/10937, Loss: 0.0691\n",
      "Epoch 4/5, Batch 2900/10937, Loss: 0.1615\n",
      "Epoch 4/5, Batch 3000/10937, Loss: 0.0511\n",
      "Epoch 4/5, Batch 3100/10937, Loss: 0.1937\n",
      "Epoch 4/5, Batch 3200/10937, Loss: 0.1294\n",
      "Epoch 4/5, Batch 3300/10937, Loss: 0.0797\n",
      "Epoch 4/5, Batch 3400/10937, Loss: 0.0774\n",
      "Epoch 4/5, Batch 3500/10937, Loss: 0.1319\n",
      "Epoch 4/5, Batch 3600/10937, Loss: 0.1839\n",
      "Epoch 4/5, Batch 3700/10937, Loss: 0.0769\n",
      "Epoch 4/5, Batch 3800/10937, Loss: 0.4106\n",
      "Epoch 4/5, Batch 3900/10937, Loss: 0.1109\n",
      "Epoch 4/5, Batch 4000/10937, Loss: 0.0407\n",
      "Epoch 4/5, Batch 4100/10937, Loss: 0.0938\n",
      "Epoch 4/5, Batch 4200/10937, Loss: 0.0831\n",
      "Epoch 4/5, Batch 4300/10937, Loss: 0.1126\n",
      "Epoch 4/5, Batch 4400/10937, Loss: 0.0411\n",
      "Epoch 4/5, Batch 4500/10937, Loss: 0.0818\n",
      "Epoch 4/5, Batch 4600/10937, Loss: 0.1115\n",
      "Epoch 4/5, Batch 4700/10937, Loss: 0.2473\n",
      "Epoch 4/5, Batch 4800/10937, Loss: 0.0763\n",
      "Epoch 4/5, Batch 4900/10937, Loss: 0.1173\n",
      "Epoch 4/5, Batch 5000/10937, Loss: 0.1502\n",
      "Epoch 4/5, Batch 5100/10937, Loss: 0.0748\n",
      "Epoch 4/5, Batch 5200/10937, Loss: 0.1315\n",
      "Epoch 4/5, Batch 5300/10937, Loss: 0.1512\n",
      "Epoch 4/5, Batch 5400/10937, Loss: 0.0671\n",
      "Epoch 4/5, Batch 5500/10937, Loss: 0.1720\n",
      "Epoch 4/5, Batch 5600/10937, Loss: 0.0935\n",
      "Epoch 4/5, Batch 5700/10937, Loss: 0.1010\n",
      "Epoch 4/5, Batch 5800/10937, Loss: 0.2026\n",
      "Epoch 4/5, Batch 5900/10937, Loss: 0.1119\n",
      "Epoch 4/5, Batch 6000/10937, Loss: 0.0471\n",
      "Epoch 4/5, Batch 6100/10937, Loss: 0.1158\n",
      "Epoch 4/5, Batch 6200/10937, Loss: 0.0973\n",
      "Epoch 4/5, Batch 6300/10937, Loss: 0.0907\n",
      "Epoch 4/5, Batch 6400/10937, Loss: 0.0446\n",
      "Epoch 4/5, Batch 6500/10937, Loss: 0.1218\n",
      "Epoch 4/5, Batch 6600/10937, Loss: 0.1268\n",
      "Epoch 4/5, Batch 6700/10937, Loss: 0.0620\n",
      "Epoch 4/5, Batch 6800/10937, Loss: 0.1153\n",
      "Epoch 4/5, Batch 6900/10937, Loss: 0.0916\n",
      "Epoch 4/5, Batch 7000/10937, Loss: 0.1230\n",
      "Epoch 4/5, Batch 7100/10937, Loss: 0.2530\n",
      "Epoch 4/5, Batch 7200/10937, Loss: 0.0627\n",
      "Epoch 4/5, Batch 7300/10937, Loss: 0.0804\n",
      "Epoch 4/5, Batch 7400/10937, Loss: 0.0701\n",
      "Epoch 4/5, Batch 7500/10937, Loss: 0.0844\n",
      "Epoch 4/5, Batch 7600/10937, Loss: 0.0849\n",
      "Epoch 4/5, Batch 7700/10937, Loss: 0.1846\n",
      "Epoch 4/5, Batch 7800/10937, Loss: 0.0970\n",
      "Epoch 4/5, Batch 7900/10937, Loss: 0.0276\n",
      "Epoch 4/5, Batch 8000/10937, Loss: 0.1052\n",
      "Epoch 4/5, Batch 8100/10937, Loss: 0.1191\n",
      "Epoch 4/5, Batch 8200/10937, Loss: 0.0588\n",
      "Epoch 4/5, Batch 8300/10937, Loss: 0.1185\n",
      "Epoch 4/5, Batch 8400/10937, Loss: 0.0938\n",
      "Epoch 4/5, Batch 8500/10937, Loss: 0.0994\n",
      "Epoch 4/5, Batch 8600/10937, Loss: 0.0474\n",
      "Epoch 4/5, Batch 8700/10937, Loss: 0.3184\n",
      "Epoch 4/5, Batch 8800/10937, Loss: 0.0983\n",
      "Epoch 4/5, Batch 8900/10937, Loss: 0.2718\n",
      "Epoch 4/5, Batch 9000/10937, Loss: 0.1789\n",
      "Epoch 4/5, Batch 9100/10937, Loss: 0.0615\n",
      "Epoch 4/5, Batch 9200/10937, Loss: 0.1621\n",
      "Epoch 4/5, Batch 9300/10937, Loss: 0.2321\n",
      "Epoch 4/5, Batch 9400/10937, Loss: 0.0521\n",
      "Epoch 4/5, Batch 9500/10937, Loss: 0.0330\n",
      "Epoch 4/5, Batch 9600/10937, Loss: 0.3567\n",
      "Epoch 4/5, Batch 9700/10937, Loss: 0.1186\n",
      "Epoch 4/5, Batch 9800/10937, Loss: 0.1512\n",
      "Epoch 4/5, Batch 9900/10937, Loss: 0.0320\n",
      "Epoch 4/5, Batch 10000/10937, Loss: 0.1029\n",
      "Epoch 4/5, Batch 10100/10937, Loss: 0.3609\n",
      "Epoch 4/5, Batch 10200/10937, Loss: 0.1650\n",
      "Epoch 4/5, Batch 10300/10937, Loss: 0.1769\n",
      "Epoch 4/5, Batch 10400/10937, Loss: 0.2338\n",
      "Epoch 4/5, Batch 10500/10937, Loss: 0.1451\n",
      "Epoch 4/5, Batch 10600/10937, Loss: 0.4123\n",
      "Epoch 4/5, Batch 10700/10937, Loss: 0.8477\n",
      "Epoch 4/5, Batch 10800/10937, Loss: 0.3356\n",
      "Epoch 4/5, Batch 10900/10937, Loss: 0.4519\n",
      "Epoch 4/5, Loss: 0.2323\n",
      "Epoch 5/5, Batch 100/10937, Loss: 0.3707\n",
      "Epoch 5/5, Batch 200/10937, Loss: 0.3743\n",
      "Epoch 5/5, Batch 300/10937, Loss: 0.3628\n",
      "Epoch 5/5, Batch 400/10937, Loss: 0.3765\n",
      "Epoch 5/5, Batch 500/10937, Loss: 0.2475\n",
      "Epoch 5/5, Batch 600/10937, Loss: 0.3466\n",
      "Epoch 5/5, Batch 700/10937, Loss: 0.3241\n",
      "Epoch 5/5, Batch 800/10937, Loss: 0.3328\n",
      "Epoch 5/5, Batch 900/10937, Loss: 0.2717\n",
      "Epoch 5/5, Batch 1000/10937, Loss: 0.2675\n",
      "Epoch 5/5, Batch 1100/10937, Loss: 0.4171\n",
      "Epoch 5/5, Batch 1200/10937, Loss: 0.3760\n",
      "Epoch 5/5, Batch 1300/10937, Loss: 0.2564\n",
      "Epoch 5/5, Batch 1400/10937, Loss: 0.3319\n",
      "Epoch 5/5, Batch 1500/10937, Loss: 0.3060\n",
      "Epoch 5/5, Batch 1600/10937, Loss: 0.2515\n",
      "Epoch 5/5, Batch 1700/10937, Loss: 0.3206\n",
      "Epoch 5/5, Batch 1800/10937, Loss: 0.2561\n",
      "Epoch 5/5, Batch 1900/10937, Loss: 0.2254\n",
      "Epoch 5/5, Batch 2000/10937, Loss: 0.2567\n",
      "Epoch 5/5, Batch 2100/10937, Loss: 0.3200\n",
      "Epoch 5/5, Batch 2200/10937, Loss: 0.3500\n",
      "Epoch 5/5, Batch 2300/10937, Loss: 0.3668\n",
      "Epoch 5/5, Batch 2400/10937, Loss: 0.3878\n",
      "Epoch 5/5, Batch 2500/10937, Loss: 0.3778\n",
      "Epoch 5/5, Batch 2600/10937, Loss: 0.5569\n",
      "Epoch 5/5, Batch 2700/10937, Loss: 0.4469\n",
      "Epoch 5/5, Batch 2800/10937, Loss: 0.4681\n",
      "Epoch 5/5, Batch 2900/10937, Loss: 0.3359\n",
      "Epoch 5/5, Batch 3000/10937, Loss: 0.3588\n",
      "Epoch 5/5, Batch 3100/10937, Loss: 0.2506\n",
      "Epoch 5/5, Batch 3200/10937, Loss: 0.2672\n",
      "Epoch 5/5, Batch 3300/10937, Loss: 0.4023\n",
      "Epoch 5/5, Batch 3400/10937, Loss: 0.4873\n",
      "Epoch 5/5, Batch 3500/10937, Loss: 0.5629\n",
      "Epoch 5/5, Batch 3600/10937, Loss: 0.5716\n",
      "Epoch 5/5, Batch 3700/10937, Loss: 0.3335\n",
      "Epoch 5/5, Batch 3800/10937, Loss: 0.6055\n",
      "Epoch 5/5, Batch 3900/10937, Loss: 0.5043\n",
      "Epoch 5/5, Batch 4000/10937, Loss: 0.5658\n",
      "Epoch 5/5, Batch 4100/10937, Loss: 0.4810\n",
      "Epoch 5/5, Batch 4200/10937, Loss: 0.4744\n",
      "Epoch 5/5, Batch 4300/10937, Loss: 0.4925\n",
      "Epoch 5/5, Batch 4400/10937, Loss: 0.5415\n",
      "Epoch 5/5, Batch 4500/10937, Loss: 0.5683\n",
      "Epoch 5/5, Batch 4600/10937, Loss: 0.5695\n",
      "Epoch 5/5, Batch 4700/10937, Loss: 0.4816\n",
      "Epoch 5/5, Batch 4800/10937, Loss: 0.7178\n",
      "Epoch 5/5, Batch 4900/10937, Loss: 0.6096\n",
      "Epoch 5/5, Batch 5000/10937, Loss: 0.5034\n",
      "Epoch 5/5, Batch 5100/10937, Loss: 0.5290\n",
      "Epoch 5/5, Batch 5200/10937, Loss: 0.5469\n",
      "Epoch 5/5, Batch 5300/10937, Loss: 0.5507\n",
      "Epoch 5/5, Batch 5400/10937, Loss: 0.5053\n",
      "Epoch 5/5, Batch 5500/10937, Loss: 0.4498\n",
      "Epoch 5/5, Batch 5600/10937, Loss: 0.4250\n",
      "Epoch 5/5, Batch 5700/10937, Loss: 0.4367\n",
      "Epoch 5/5, Batch 5800/10937, Loss: 0.4497\n",
      "Epoch 5/5, Batch 5900/10937, Loss: 0.5662\n",
      "Epoch 5/5, Batch 6000/10937, Loss: 0.5706\n",
      "Epoch 5/5, Batch 6100/10937, Loss: 0.6348\n",
      "Epoch 5/5, Batch 6200/10937, Loss: 0.5372\n",
      "Epoch 5/5, Batch 6300/10937, Loss: 0.5442\n",
      "Epoch 5/5, Batch 6400/10937, Loss: 0.5457\n",
      "Epoch 5/5, Batch 6500/10937, Loss: 0.5723\n",
      "Epoch 5/5, Batch 6600/10937, Loss: 0.5281\n",
      "Epoch 5/5, Batch 6700/10937, Loss: 0.3404\n",
      "Epoch 5/5, Batch 6800/10937, Loss: 0.4633\n",
      "Epoch 5/5, Batch 6900/10937, Loss: 0.4717\n",
      "Epoch 5/5, Batch 7000/10937, Loss: 0.5806\n",
      "Epoch 5/5, Batch 7100/10937, Loss: 0.4669\n",
      "Epoch 5/5, Batch 7200/10937, Loss: 0.2603\n",
      "Epoch 5/5, Batch 7300/10937, Loss: 0.4538\n",
      "Epoch 5/5, Batch 7400/10937, Loss: 0.1233\n",
      "Epoch 5/5, Batch 7500/10937, Loss: 0.5977\n",
      "Epoch 5/5, Batch 7600/10937, Loss: 0.1041\n",
      "Epoch 5/5, Batch 7700/10937, Loss: 0.1355\n",
      "Epoch 5/5, Batch 7800/10937, Loss: 0.5322\n",
      "Epoch 5/5, Batch 7900/10937, Loss: 0.5704\n",
      "Epoch 5/5, Batch 8000/10937, Loss: 0.0669\n",
      "Epoch 5/5, Batch 8100/10937, Loss: 0.6048\n",
      "Epoch 5/5, Batch 8200/10937, Loss: 0.4269\n",
      "Epoch 5/5, Batch 8300/10937, Loss: 0.0225\n",
      "Epoch 5/5, Batch 8400/10937, Loss: 0.5425\n",
      "Epoch 5/5, Batch 8500/10937, Loss: 0.5081\n",
      "Epoch 5/5, Batch 8600/10937, Loss: 0.4655\n",
      "Epoch 5/5, Batch 8700/10937, Loss: 0.5041\n",
      "Epoch 5/5, Batch 8800/10937, Loss: 0.4408\n",
      "Epoch 5/5, Batch 8900/10937, Loss: 0.2764\n",
      "Epoch 5/5, Batch 9000/10937, Loss: 0.2085\n",
      "Epoch 5/5, Batch 9100/10937, Loss: 0.3379\n",
      "Epoch 5/5, Batch 9200/10937, Loss: 0.2153\n",
      "Epoch 5/5, Batch 9300/10937, Loss: 0.4169\n",
      "Epoch 5/5, Batch 9400/10937, Loss: 0.4415\n",
      "Epoch 5/5, Batch 9500/10937, Loss: 0.2495\n",
      "Epoch 5/5, Batch 9600/10937, Loss: 0.2492\n",
      "Epoch 5/5, Batch 9700/10937, Loss: 0.5727\n",
      "Epoch 5/5, Batch 9800/10937, Loss: 0.6230\n",
      "Epoch 5/5, Batch 9900/10937, Loss: 0.2963\n",
      "Epoch 5/5, Batch 10000/10937, Loss: 0.5479\n",
      "Epoch 5/5, Batch 10100/10937, Loss: 0.3179\n",
      "Epoch 5/5, Batch 10200/10937, Loss: 0.5194\n",
      "Epoch 5/5, Batch 10300/10937, Loss: 0.4609\n",
      "Epoch 5/5, Batch 10400/10937, Loss: 0.6507\n",
      "Epoch 5/5, Batch 10500/10937, Loss: 0.3763\n",
      "Epoch 5/5, Batch 10600/10937, Loss: 0.6164\n",
      "Epoch 5/5, Batch 10700/10937, Loss: 0.5539\n",
      "Epoch 5/5, Batch 10800/10937, Loss: 0.5292\n",
      "Epoch 5/5, Batch 10900/10937, Loss: 0.5644\n",
      "Epoch 5/5, Loss: 0.4453\n",
      "Test Accuracy: 78.55%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 5. Initialize Model, Loss, and Optimizer\n",
    "input_dim = X_train.shape[2]  # Number of features\n",
    "patch_size = 10\n",
    "d_model = 64\n",
    "n_heads = 4\n",
    "num_layers = 3\n",
    "num_classes = len(np.unique(y))  # Number of unique labels (0-7)\n",
    "dropout = 0.1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = PatchTST(input_dim, patch_size, d_model, n_heads, num_layers, num_classes, dropout).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# 6. Training Loop\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch_idx, (X_batch, y_batch) in enumerate(train_loader):\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Print update every 10 batches\n",
    "        if (batch_idx + 1) % 100 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Batch {batch_idx+1}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {train_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# 7. Evaluate Model\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        outputs = model(X_batch)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(y_batch.cpu().numpy())\n",
    "        total += y_batch.size(0)\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "# 8. Confusion Matrix and Visualization\n",
    "\n",
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds, labels=np.unique(all_labels))\n",
    "\n",
    "# Static confusion matrix (Matplotlib)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.unique(all_labels))\n",
    "disp.plot(cmap='viridis', xticks_rotation='vertical')\n",
    "plt.title(\"Confusion Matrix (Matplotlib)\")\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
